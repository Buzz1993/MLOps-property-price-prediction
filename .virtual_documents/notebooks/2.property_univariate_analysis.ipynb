


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib import gridspec
import seaborn as sns
import scipy.stats as stats
from sklearn.preprocessing import (
    PowerTransformer
)











pd.set_option('display.max_rows',None)
pd.set_option('display.max_colwidth',None)
# Instead of getting output as NumPy array, get it as a pandas DataFrame
from sklearn import set_config
set_config(transform_output="pandas")





df = pd.read_csv(r'C:\Users\ABC\Desktop/bhushan/property_project/data/property_cleaned_data.csv')


df.head()


df.shape





# check for missing values in the data

df.isnull().sum()


# number of rows in the data that have missing values

missing_rows = (
    df
    .isnull()
    .any(axis=1)
    .sum()
)

print(f'There are {missing_rows} rows with missing values in the data.')

print(f"It accounts for {(missing_rows/df.shape[0])*100:.2f}% of the data")


# check for duplicate rows in the data

df.duplicated().sum()





# data types of features

df.dtypes


#Description of our dataset
df.describe().T


#Description of our dataset
df.describe(include='O').T


#find the numerical columns
numerical_columns = df.select_dtypes(include=['number']).columns
print(numerical_columns)
print("-"*100)
print(f'total numerical columns : {len(numerical_columns)}')


categorical_columns = df.select_dtypes(include=['object', 'category']).columns
print(categorical_columns)
print("-" * 100)
print(f'Total categorical columns: {len(categorical_columns)}')








def analyze_column(df, feature, top_n=None, threshold=None):
    import warnings
    warnings.filterwarnings("ignore")

    col = df.loc[:, feature].copy()

    print('-' * 50)
    print(f"Quick Glance of '{feature}':")
    display(col.head())

    print('-' * 50)
    print(f"Meta-data for '{feature}':")
    print(f"{'Data Type':15}: {col.dtype}")
    print(f"{'Cardinality':15}: {col.nunique(dropna=True)}") #prints the number of unique (non-NaN) values
    print(f"{'Missing Data':15}: {col.isna().sum():,} rows ({col.isna().mean() * 100:.2f} %)")
    print(f"{'Available Data':15}: {col.count():,} / {len(col):,} rows")

    if col.dtype in ["float64", "int64"]:  # Numerical column
        print('-' * 50)
        print(f"Descriptive Statistics for '{feature}':")
        display(col.describe())

        print('-' * 50)
        print(f"Top Frequent Values in '{feature}':")

        value_counts = col.value_counts().to_frame(name='count')
        total = col.count()  #Use non-null count to get valid total
        value_counts['percentage'] = (value_counts['count'] / total * 100).round(2)
        value_counts['cum_percent'] = value_counts['percentage'].cumsum().round(2)

        # Filter by threshold (before top_n)
        if threshold is not None:
            value_counts = value_counts[value_counts['cum_percent'] <= threshold]

        # Apply top_n filter if given
        if top_n is not None:
            display(value_counts.head(top_n))
        else:
            display(value_counts)

        # After displaying, print the values list
        if threshold is not None:
            value_list = value_counts.index.tolist()
            print(f"\n{len(value_list)} Values in '{feature}' with cumulative percentage ≤ {threshold}%:")
            print(value_list)






# Categorical summary function with top_n and threshold
def cat_summary(data, var, top_n=None, threshold=None):
    import pandas as pd
    import warnings
    warnings.filterwarnings("ignore")

    col = data.loc[:, var].copy()

    # Quick Glance
    print('-'*50)
    print('Quick Glance:')
    display(col.head())

    # Meta-data
    print('-'*50)
    print('Meta-data')
    print(f"{'Data Type':15}: {col.dtype}")
    print(f"{'Cardinality':15}: {col.nunique(dropna=True)} categories")
    print(f"{'Missing Data':15}: {col.isna().sum():,} rows ({col.isna().mean() * 100:.2f} %)")
    print(f"{'Available Data':15}: {col.count():,} / {len(col):,} rows")

    # Summary
    print('-'*50)
    print('summary:')
    display(col.describe().rename("").to_frame())

    # Category Distribution
    print('-'*50)
    print('Categories Distribution:')
    vc = col.value_counts()
    total = col.count()  #Use count() to exclude NaN from percentage calculations

    percentage = (vc / total * 100).round(2)
    cum_percent = percentage.cumsum().round(2)

    vc_df = pd.DataFrame({
        "count": vc,
        "percentage": percentage,
        "cum_percent": cum_percent
    })
    vc_df.index.name = "category"

    # Apply threshold first
    if threshold is not None:
        vc_df = vc_df[vc_df['cum_percent'] <= threshold]

    # Apply top_n filter
    if top_n is not None:
        vc_df = vc_df.head(top_n)

    display(vc_df)

    # Print values within threshold
    if threshold is not None:
        values_list = vc_df.index.tolist()
        print(f"\n{len(values_list)} Categories in '{var}' with cumulative percentage ≤ {threshold}%:")
        print(values_list)






def num_three_chart_plot(df, feature):
    fig = plt.figure(constrained_layout=True, figsize=(20, 8))
    grid = gridspec.GridSpec(ncols=4, nrows=6, figure=fig)

    ax1 = fig.add_subplot(grid[0:3, :3])  # Increase histogram height
    ax1.set_title('Histogram')
    sns.histplot(df[feature], kde=True, ax=ax1)
    ax1.axvline(x=df[feature].mean(), c='red', label="Mean")
    ax1.axvline(x=df[feature].median(), c='green', label="Median")
    ax1.legend()

    ax2 = fig.add_subplot(grid[3:6, :3])  # Increase probability plot height
    ax2.set_title('QQ Plot')
    stats.probplot(df[feature], plot=ax2)

    ax3 = fig.add_subplot(grid[:, 3])
    ax3.set_title('Box Plot')
    sns.boxplot(y=df[feature], ax=ax3)

    plt.show()





def bar_pie_chart_plot(df, feature, bar_top_n=None, pie_top_n=None):
    fig = plt.figure(constrained_layout=True, figsize=(25, 8))
    grid = gridspec.GridSpec(ncols=2, nrows=1, figure=fig)  # 2 columns: Bar plot + Pie chart

    # Get value counts and sort in descending order
    value_counts = df[feature].value_counts().sort_values(ascending=False)

    # Bar Chart Data (Apply bar_top_n filter if specified)
    bar_data = value_counts.reset_index()
    if bar_top_n is not None and bar_top_n < len(value_counts):
        bar_data = bar_data.iloc[:bar_top_n]
    bar_data.columns = [feature, 'count']

    # Pie Chart Data (Top N categories + "Others" only if needed)
    if pie_top_n is not None and pie_top_n < len(value_counts):
        top_values = value_counts.iloc[:pie_top_n]  # Keep only top N categories
        others_sum = value_counts.iloc[pie_top_n:].sum()  # Sum of remaining categories
        pie_data = pd.concat([top_values, pd.Series({'Others': others_sum})])  # Add "Others"
        pie_data = pie_data.reset_index()
        pie_data.columns = [feature, 'count']
    else:
        pie_data = value_counts.reset_index()
        pie_data.columns = [feature, 'count']

    pie_data = pie_data.sort_values(by="count", ascending=False)  # Ensure sorting

    # Bar Chart
    ax1 = fig.add_subplot(grid[0, 0])
    ax1.set_title(f'{feature} Distribution - Bar Chart (Top {bar_top_n if bar_top_n else "All"})', fontsize=14)

    sns.barplot(x=feature, y='count', data=bar_data, ax=ax1, order=bar_data[feature], 
                palette='tab10', edgecolor='black')

    # Add count labels on top of bars
    for p in ax1.patches:
        ax1.annotate(f'{int(p.get_height())}', 
                     (p.get_x() + p.get_width() / 2, p.get_height()), 
                     ha='center', va='bottom', fontsize=12, color='black')

    ax1.set_xticks(range(len(bar_data[feature])))  # Explicitly set tick positions
    ax1.set_xticklabels(bar_data[feature], rotation=45, ha='right', fontsize=10)
    ax1.grid(axis="y", linestyle="--", alpha=0.7)
    ax1.set_ylabel("Count")

    # Pie Chart
    ax2 = fig.add_subplot(grid[0, 1])
    ax2.set_title(f'{feature} Distribution - Pie Chart (Top {pie_top_n if pie_top_n else "All"})', fontsize=14)

    ax2.pie(pie_data['count'], labels=pie_data[feature], autopct='%1.1f%%', colors=sns.color_palette('tab10'), 
            wedgeprops={'edgecolor': 'black'},radius=1.1)

    plt.show()





# gives detailed summary of numeric features
def num_summary(data, var):
  import warnings
  warnings.filterwarnings("ignore")

  col = data.loc[:, var].copy()

  # quantiles
  print('-'*50)
  print('Percentiles:')  
  display(
      col
      .quantile([0.0, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99, 1.0])
      .rename(index=lambda val: f"{val * 100:.0f}")
      .rename("value")
      .rename_axis(index="percentile")
      .to_frame()
  )

  # central tendancy
  print('-'*50)
  print('Central Tendancy:')
  display(
      pd
      .Series({
          "mean": col.mean(),
          "trimmed mean (5%)": stats.trim_mean(col.dropna().values, 0.05),
          "trimmed mean (10%)": stats.trim_mean(col.dropna().values, 0.1),
          "median": col.median()
      })
      .rename("value")
      .to_frame()
  )

  # spread
  print('-'*50)
  print('Measure of Spread:')
  std = col.std()
  q1 = col.quantile(0.25)
  q3 = col.quantile(0.75)
  iqr = q3 - q1
  lower_bound = q1 - 1.5 * iqr
  upper_bound = q3 + 1.5 * iqr
  display(
      pd
      .Series({
          "var": col.var(),
          "std": std,
          "IQR": iqr,
          "lower_bound": lower_bound,
          "upper_bound": upper_bound,
          "mad": stats.median_abs_deviation(col.dropna()),
          "coef_variance": std / col.mean()
      })
      .rename("value")
      .to_frame()
  )

  # skewness and kurtosis
  print('-'*50)
  print('Skewness and Kurtosis:')
  display(
      pd
      .Series({
          "skewness": col.skew(),
          "kurtosis": col.kurtosis()
      })
      .rename("value")
      .to_frame()
  )

  alpha = 0.05
  # test for normality to chk numerical columns follows the normal distribution or not 
  print('-'*50)
  print('Hypothesis Testing for Normality:')

  from scipy.stats import jarque_bera

  # Jarque-Bera Test
  print('Jarque-Bera Test:')
  jb_test = jarque_bera(col.dropna().values)
  jb_statistic = jb_test.statistic
  jb_pvalue = jb_test.pvalue

  alpha = 0.05  # significance level

  print(f"{'Significance Level':21}: {alpha}")
  print(f"{'Null Hypothesis':21}: The data is normally distributed")
  print(f"{'Alternate Hypothesis':21}: The data is not normally distributed")
  print(f"{'p-value':21}: {jb_pvalue}")
  print(f"{'Test Statistic':21}: {jb_statistic}")

  if jb_pvalue < alpha:
      print(f"- Since p-value is less than alpha ({alpha}), we Reject the Null Hypothesis at {alpha * 100}% significance level")
      print("- CONCLUSION: We conclude that the data sample is not normally distributed")
  else:
      print(f"- Since p-value is greater than alpha ({alpha}), we Fail to Reject the Null Hypothesis at {alpha * 100}% significance level")
      print("- CONCLUSION: We conclude that the data sample is normally distributed")


  # anderson-darling test
  print('-'*50)
  print('Anderson-Darling Test:')
  ad_test = stats.anderson(col.dropna().values, dist="norm")
  ad_statistic = ad_test.statistic
  ad_critical = ad_test.critical_values[2]
  print(f"{'Significance Level':21}: {alpha}")
  print(f"{'Null Hypothesis':21}: The data is normally distributed")
  print(f"{'Alternate Hypothesis':21}: The data is not normally distributed")
  print(f"{'Critical Value':21}: {ad_critical}")
  print(f"{'Test Statistic':21}: {ad_statistic}")
  if ad_statistic >= ad_critical:
    print(f"- Since the Test-statistic is greater than Critical Value, we Reject the Null Hypothesis at {alpha * 100}% significance level")
    print("- CONCLUSION: We conclude that the data sample is not normally distributed")
  else:
    print(f"- Since the Test-statistic is less than Critical Value, we Fail to Reject the Null Hypothesis at {alpha * 100}% significance level")
    print("- CONCLUSION: We conclude that the data sample is normally distributed")











# hypothesis testing for association between numeric and categorical variable
def num_cat_hyp_testing(data, num_var, cat_var, alpha=0.05):
  display_html(2, f"Hypothesis Test for Association between {num_var} and {cat_var}")

  groups_df = (
      data
      .dropna(subset=[num_var])
      .groupby(cat_var)
  )
  groups = [group[num_var].values for _, group in groups_df]

  # anova test
  anova = stats.f_oneway(*groups)
  statistic = anova[0]
  pvalue = anova[1]
  display_html(3, "ANOVA Test")
  print(f"- {'Significance Level':21}: {alpha * 100}%")
  print(f"- {'Null Hypothesis':21}: The groups have similar population mean")
  print(f"- {'Alternate Hypothesis':21}: The groups don't have similar population mean")
  print(f"- {'Test Statistic':21}: {statistic}")
  print(f"- {'p-value':21}: {pvalue}")
  if pvalue < alpha:
    print(f"- Since p-value is less than {alpha}, we Reject the Null Hypothesis at {alpha * 100}% significance level")
    print(f"- CONCLUSION: The variables {num_var} and {cat_var} are associated to each other")
  else:
    print(f"- Since p-value is greater than {alpha}, we Fail to Reject the Null Hypothesis at {alpha * 100}% significance level")
    print(f"- CONCLUSION: The variables {num_var} and {cat_var} are not associated to each other")

  # kruskal-wallis test
  kruskal = stats.kruskal(*groups)
  statistic = kruskal[0]
  pvalue = kruskal[1]
  display_html(3, "Kruskal-Wallis Test")
  print(f"- {'Significance Level':21}: {alpha * 100}%")
  print(f"- {'Null Hypothesis':21}: The groups have similar population median")
  print(f"- {'Alternate Hypothesis':21}: The groups don't have similar population median")
  print(f"- {'Test Statistic':21}: {statistic}")
  print(f"- {'p-value':21}: {pvalue}")
  if pvalue < alpha:
    print(f"- Since p-value is less than {alpha}, we Reject the Null Hypothesis at {alpha * 100}% significance level")
    print(f"- CONCLUSION: The variables {num_var} and {cat_var} are associated to each other")
  else:
    print(f"- Since p-value is greater than {alpha}, we Fail to Reject the Null Hypothesis at {alpha * 100}% significance level")
    print(f"- CONCLUSION: The variables {num_var} and {cat_var} are not associated to each other")



def num_univar_plots(data, var, bins=10, figsize=(13, 8)):

    col = data.loc[:, var].copy()

    fig, axes = plt.subplots(2, 2, figsize=figsize)
    axes = axes.ravel()

    # Histogram
    sns.histplot(
        data,
        x=var,
        bins=bins,
        kde=True,
        color="#1973bd",
        ax=axes[0],
    )
    sns.rugplot(
        data,
        x=var,
        color="black",
        height=0.035,
        ax=axes[0]
    )
    axes[0].set(title="Histogram")

    # CDF
    sns.ecdfplot(
        data,
        x=var,
        ax=axes[1],
        color="red"
    )
    axes[1].set(title="CDF")

    # Power Transform
    data = data.assign(**{
        f"{var}_pwt": (
            PowerTransformer()
            .fit_transform(data.loc[:, [var]])
        )
    })
    sns.kdeplot(
        data,
        x=f"{var}_pwt",
        fill=True,
        color="#f2b02c",
        ax=axes[2]
    )
    sns.rugplot(
        data,
        x=f"{var}_pwt",
        color="black",
        height=0.035,
        ax=axes[2]
    )
    axes[2].set(title="Power Transformed")  #by default it use yeo-johnson transform 

    # Violin Plot
    sns.violinplot(
        data,
        x=var,
        color="#ed68b4",
        ax=axes[3]
    )
    axes[3].set(title="Violin Plot")

    plt.tight_layout()
    plt.show()









analyze_column(df, 'price', top_n=5,threshold=20)





bar_pie_chart_plot(df, 'price', bar_top_n=40, pie_top_n=10)


num_three_chart_plot(df, 'price')





num_univar_plots(df, 'price', bins=10)





plt.figure(figsize=(15, 6))

# Distribution plot without log transformation
plt.subplot(1, 2, 1)
sns.histplot(df['price'], kde=True, bins=50, color='skyblue')
plt.title('Distribution of Prices (Original)')
plt.xlabel('Price (in Crores)')
plt.ylabel('Frequency')

# Distribution plot with log transformation
plt.subplot(1, 2, 2)
sns.histplot(np.log1p(df['price']), kde=True, bins=50, color='lightgreen')
plt.title('Distribution of Prices (Log Transformed)')
plt.xlabel('Log(Price)')
plt.ylabel('Frequency')

plt.tight_layout()
plt.show()





num_summary(df,'price')





bins = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, float('inf')]
labels = [
    '0.00-0.99', '1.00-1.99', '2.00-2.99', '3.00-3.99',
    '4.00-4.99', '5.00-5.99', '6.00-6.99', '7.00-7.99',
    '8.00-8.99', '9.00-9.99', '10.00-14.99',
    '15.00-20.00', '20.00 and above'
]

df['price_category'] = pd.cut(df['price'], bins=bins, labels=labels, right=False)

counts = df['price_category'].value_counts().reindex(labels, fill_value=0)

counts.plot(kind='bar', figsize=(12, 6))
plt.xticks(rotation=45)
plt.title('Price Category Distribution')
plt.xlabel('Price Category')
plt.ylabel('Count')
plt.tight_layout()
plt.show()






# Check why extreme values in the 'price' column are coming up as outliers

target_25_per, target_75_per = np.percentile(df['price'], [25, 75])
iqr = target_75_per - target_25_per

upper_bound = target_75_per + (1.5 * iqr)

# Let’s check if the 'area' column values are causing these outlier values in the 'price' column
df.loc[(df['price'] > upper_bound), 'city'].value_counts()


df['city'].value_counts()








#find the numerical columns
numerical_columns = df.select_dtypes(include=['number']).columns
print(numerical_columns)
print("-"*100)
print(f'total numerical columns : {len(numerical_columns)}')





# no of unique categories and value_counts
analyze_column(df, 'project_in_acres', top_n=20, threshold=90)





#bar chart and pie plot 
bar_pie_chart_plot(df, 'project_in_acres', bar_top_n=45, pie_top_n=10)


num_three_chart_plot(df, 'project_in_acres')





num_univar_plots(df, 'project_in_acres', bins=10)


num_summary(df,'project_in_acres')








# no of unique categories and value_counts
analyze_column(df, 'lattitude', top_n=5,threshold=20)





bar_pie_chart_plot(df, 'lattitude', bar_top_n=25, pie_top_n=8)


num_three_chart_plot(df, 'lattitude')





num_univar_plots(df, 'lattitude', bins=10)


num_summary(df,'lattitude')








# no of unique categories and value_counts
analyze_column(df, 'longitude', top_n=5,threshold=20)





bar_pie_chart_plot(df, 'longitude', bar_top_n=25, pie_top_n=8)


num_three_chart_plot(df, 'longitude')





num_univar_plots(df, 'longitude', bins=10)


num_summary(df,'longitude')








# no of unique categories and value_counts
analyze_column(df, 'flat_on_floor', top_n=5,threshold=20)





#bar chart and pie plot 
bar_pie_chart_plot(df, 'flat_on_floor', bar_top_n=50, pie_top_n=20)


num_three_chart_plot(df, 'flat_on_floor')





num_univar_plots(df, 'flat_on_floor', bins=10)


 num_summary(df,'flat_on_floor')








# no of unique categories and value_counts
analyze_column(df, 'total_floor', top_n=5,threshold=20)





#bar chart and pie plot 
bar_pie_chart_plot(df, 'total_floor', bar_top_n=45, pie_top_n=14)


num_three_chart_plot(df, 'total_floor')





num_univar_plots(df, 'total_floor', bins=10)


num_summary(df,'total_floor')








# no of unique categories and value_counts
analyze_column(df, 'area', top_n=5,threshold=20)





bar_pie_chart_plot(df, 'area', bar_top_n=30, pie_top_n=8)


num_three_chart_plot(df, 'area')





num_univar_plots(df, 'area', bins=10)


num_summary(df,'area')








# no of unique categories and value_counts
analyze_column(df, 'bed', top_n=5,threshold=20)





#bar chart and pie plot 
bar_pie_chart_plot(df, 'bed', bar_top_n=9, pie_top_n=5)


num_three_chart_plot(df, 'bed')


num_univar_plots(df, 'bed', bins=10)


num_summary(df,'bed')








# no of unique categories and value_counts
analyze_column(df, 'bath', top_n=5,threshold=20)





#bar chart and pie plot 
bar_pie_chart_plot(df, 'bath', bar_top_n=10, pie_top_n=5)


num_three_chart_plot(df, 'bath')





num_univar_plots(df, 'bath', bins=10)


num_summary(df,'bath')








# no of unique categories and value_counts
analyze_column(df, 'balcony', top_n=5,threshold=20)





#bar chart and pie plot 
bar_pie_chart_plot(df, 'balcony', bar_top_n=8, pie_top_n=5)


num_three_chart_plot(df, 'balcony')





num_univar_plots(df, 'balcony', bins=10)


num_summary(df,'balcony')








# no of unique categories and value_counts
analyze_column(df, 'lift', top_n=5,threshold=20)





#bar chart and pie plot 
bar_pie_chart_plot(df, 'lift', bar_top_n=10, pie_top_n=10) 





num_three_chart_plot(df, 'lift')





num_univar_plots(df, 'lift', bins=10)


num_summary(df,'lift')








# no of unique categories and value_counts
analyze_column(df, 'parking', top_n=5,threshold=100)





#bar chart and pie plot 
bar_pie_chart_plot(df, 'parking', bar_top_n=10, pie_top_n=4)





num_three_chart_plot(df, 'parking')





num_univar_plots(df, 'parking', bins=10)


num_summary(df,'parking')








# no of unique categories and value_counts
analyze_column(df, 'costpersqft', top_n=5,threshold=20)





bar_pie_chart_plot(df, 'costpersqft', bar_top_n=30, pie_top_n=5)


num_three_chart_plot(df, 'costpersqft')





num_univar_plots(df, 'costpersqft', bins=10)


num_summary(df,'costpersqft')








# no of unique categories and value_counts
analyze_column(df, 'emi', top_n=5,threshold=20)





bar_pie_chart_plot(df, 'emi', bar_top_n=25, pie_top_n=8)


num_three_chart_plot(df, 'emi')





num_univar_plots(df, 'emi', bins=10)


num_summary(df,'emi')








# Basic summary
distance_cols = [
    'education_mean_km', 'education_min_km', 'education_within_2km',
    'transport_mean_km', 'transport_min_km', 'transport_within_2km',
    'shopping_centre_mean_km', 'shopping_centre_min_km', 'shopping_centre_within_2km',
    'commercial_hub_mean_km', 'commercial_hub_min_km', 'commercial_hub_within_2km',
    'hospital_mean_km', 'hospital_min_km', 'hospital_within_2km',
    'tourist_mean_km', 'tourist_min_km', 'tourist_within_2km',
    'overall_min_mean_km', 'overall_avg_mean_km',
    'overall_min_min_km', 'overall_avg_min_km', 'total_within_2km'
]


# Split into categories
mean_cols = [col for col in distance_cols if 'mean_km' in col and 'overall' not in col] 
min_cols  = [col for col in distance_cols if 'min_km' in col and 'overall' not in col]
within_cols = [col for col in distance_cols if 'within_2km' in col or col == 'total_within_2km']
overall_cols = ['overall_min_mean_km', 'overall_avg_mean_km', 'overall_min_min_km', 'overall_avg_min_km']

print("Mean columns:", mean_cols)
print("Min columns:", min_cols)
print("Within 2km columns:", within_cols)
print('overall_cols:', overall_cols)


# 1) Summary
df[distance_cols].describe().transpose()

# 2) Check negatives
(df[distance_cols] < 0).sum()

df[mean_cols].plot(kind='box', figsize=(12, 6))
plt.title('Boxplot: Mean Distances')
plt.xticks(rotation=45)
plt.show()


# Or histogram for one group
df[mean_cols].plot(kind='hist', bins=30, alpha=0.7, figsize=(12, 6))
plt.title('Histogram: Mean Distances')
plt.show()






#print(df['id']) where df['education_mean_km']

df['education_mean_km'].isna().sum()


df[min_cols].plot(kind='box', figsize=(12, 6))
plt.title('Boxplot: Mean Distances')
plt.xticks(rotation=45)
plt.show()


# Or histogram for one group
df[min_cols].plot(kind='hist', bins=30, alpha=0.7, figsize=(12, 6))
plt.title('Histogram: Mean Distances')
plt.show()



df[within_cols].plot(kind='box', figsize=(12, 6))
plt.title('Boxplot: Mean Distances')
plt.xticks(rotation=45)
plt.show()


# Or histogram for one group
df[within_cols].plot(kind='hist', bins=30, alpha=0.7, figsize=(12, 6))
plt.title('Histogram: Mean Distances')
plt.show()



# Boxplot
df[overall_cols].plot(kind='box', figsize=(12, 6))
plt.title('Boxplot: Overall Distance Features')
plt.xticks(rotation=45)
plt.show()

# Histogram
df[overall_cols].plot(kind='hist', bins=30, alpha=0.7, figsize=(12, 6))
plt.title('Histogram: Overall Distance Features')
plt.show()



# Split columns
km_cols = [col for col in distance_cols if 'mean_km' in col or 'min_km' in col or 'overall' in col]
flag_cols = [col for col in distance_cols if 'within_2km' in col]

# Summary statistics for distance (numerical) columns
print("\n--- Summary Statistics for Distance Columns ---")
print(df[km_cols].describe().T)

# Skewness & kurtosis
print("\n--- Skewness & Kurtosis ---")
for col in km_cols:
    skew = df[col].skew()
    kurt = df[col].kurtosis()
    print(f"{col:30s} | Skew: {skew:.2f} | Kurtosis: {kurt:.2f}")

summary_zero_nan = pd.DataFrame({
    'Zero_Counts': (df[distance_cols] == 0).sum(),
    'NaN_Counts': df[distance_cols].isna().sum()
}).sort_values(by='Zero_Counts', ascending=False)

print("\n--- Zero KM and NaN Counts ---")
print(summary_zero_nan)

# Plot histograms and boxplots
for col in km_cols:
    fig, axes = plt.subplots(1, 2, figsize=(12, 4))
    sns.histplot(df[col].dropna(), kde=True, ax=axes[0])
    axes[0].set_title(f"Histogram of {col}")
    
    sns.boxplot(x=df[col], ax=axes[1])
    axes[1].set_title(f"Boxplot of {col}")
    plt.tight_layout()
    plt.show()

# Binary/flag columns: value counts and bar plots
print("\n--- Value Counts for 'within_2km' Columns ---")
for col in flag_cols:
    print(f"\n{col}")
    print(df[col].value_counts(dropna=False))

    # Bar plot
    sns.countplot(x=df[col])
    plt.title(f"Bar Plot of {col}")
    plt.show()








categorical_columns = df.select_dtypes(include=['object', 'category']).columns
print(categorical_columns)
print("-" * 100)
print(f'Total categorical columns: {len(categorical_columns)}')





cat_summary(df, 'builder', top_n=10, threshold=90) 


bar_pie_chart_plot(df, 'builder', bar_top_n=25, pie_top_n=8)


project_counts = df['builder'].value_counts()

# Frequency distribution for societies
frequency_bins = {
    "High (50-100)": int(((project_counts >= 50) & (project_counts <= 100)).sum()),
    "Average (10-49)": int(((project_counts >= 10) & (project_counts < 50)).sum()),
    "Low (2-9)": int(((project_counts > 1) & (project_counts < 10)).sum()),
    "Very Low (1)": int((project_counts == 1).sum())
}
frequency_bins








cat_summary(df, 'project_name', top_n=5, threshold=90)


#bar chart and pie plot 
bar_pie_chart_plot(df, 'project_name', bar_top_n=40, pie_top_n=7)





society_counts = df['project_name'].value_counts()

# Frequency distribution for societies
frequency_bins = {
    "High (50-100)": int(((society_counts >= 50) & (society_counts <= 100)).sum()),
    "Average (10-49)": int(((society_counts >= 10) & (society_counts < 50)).sum()),
    "Low (2-9)": int(((society_counts > 1) & (society_counts < 10)).sum()),
    "Very Low (1)": int((society_counts == 1).sum())
}
frequency_bins








cat_summary(df, 'location', top_n=13, threshold=90)


#bar chart and pie plot 
bar_pie_chart_plot(df, 'location', bar_top_n=20, pie_top_n=13)





cat_summary(df, 'city', top_n=4, threshold=90)


#bar chart and pie plot 
bar_pie_chart_plot(df, 'city', bar_top_n=4, pie_top_n=4)








cat_summary(df, 'property_type', top_n=2, threshold=90)


#bar chart and pie plot 
bar_pie_chart_plot(df, 'property_type', bar_top_n=4, pie_top_n=4)








cat_summary(df, 'status', top_n=3, threshold=90)


#bar chart and pie plot 
bar_pie_chart_plot(df, 'status', bar_top_n=3, pie_top_n=3)








cat_summary(df, 'construction', top_n=8, threshold=100)  


#bar chart and pie plot 
bar_pie_chart_plot(df, 'construction', bar_top_n=7, pie_top_n=7)








cat_summary(df, 'ownership', top_n=4, threshold=90)


#bar chart and pie plot 
bar_pie_chart_plot(df, 'ownership', bar_top_n=4, pie_top_n=4)











cat_summary(df, 'furnish', top_n=3, threshold=90)


#bar chart and pie plot 
bar_pie_chart_plot(df, 'furnish', bar_top_n=3, pie_top_n=3)








cat_summary(df, 'overlooking', top_n=7, threshold=90)


#bar chart and pie plot 
bar_pie_chart_plot(df, 'overlooking', bar_top_n=7, pie_top_n=5)








cat_summary(df, 'facing', top_n=8, threshold=90)


#bar chart and pie plot 
bar_pie_chart_plot(df, 'facing', bar_top_n=8, pie_top_n=8)








cat_summary(df, 'extra_rooms', top_n=16, threshold=90)


#bar chart and pie plot 
bar_pie_chart_plot(df, 'extra_rooms', bar_top_n=16, pie_top_n=9)








cat_summary(df, 'price_category', top_n=13, threshold=90)


#bar chart and pie plot 
bar_pie_chart_plot(df, 'price_category', bar_top_n=13, pie_top_n=13)








cat_summary(df, 'seller', top_n=10, threshold=100)


#bar chart and pie plot 
bar_pie_chart_plot(df, 'seller', bar_top_n=3, pie_top_n=3)





df.shape


df.columns

































































































































































def fill_na_with_group_min(df, col, group_cols):
    """
    Fill NaN values in `col` using group-wise minimums based on the provided `group_cols` list.
    The function applies each group level in order. Skips if the group key is NaN.
    """
    for group_col in group_cols:
        group_min = df.groupby(group_col, dropna=True)[col].min()
        df[col] = df.apply(
            lambda row: (
                group_min[row[group_col]] if pd.isna(row[col]) and pd.notna(row[group_col])
                else row[col]
            ),
            axis=1
        )
    return df

# Example usage for multiple distance columns
distance_cols = [
    'education_mean_km', 'education_min_km', 'education_within_2km',
    'transport_mean_km', 'transport_min_km', 'transport_within_2km',
    'shopping_centre_mean_km', 'shopping_centre_min_km', 'shopping_centre_within_2km',
    'commercial_hub_mean_km', 'commercial_hub_min_km', 'commercial_hub_within_2km',
    'hospital_mean_km', 'hospital_min_km', 'hospital_within_2km',
    'tourist_mean_km', 'tourist_min_km', 'tourist_within_2km',
    'overall_min_mean_km', 'overall_avg_mean_km',
    'overall_min_min_km', 'overall_avg_min_km', 'total_within_2km'
]

# Apply for each distance column
for col in distance_cols:
    df = fill_na_with_group_min(df, col, group_cols=['location', 'project_name'])




































































































